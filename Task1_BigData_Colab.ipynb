{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arati930/codtech_tasks/blob/main/Task1_BigData_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "479367a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "479367a2",
        "outputId": "32c52f1e-efca-490f-b7fa-a2741ce2ecfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,065 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,810 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,326 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,738 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 20.0 MB in 5s (3,867 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install Java (needed for PySpark) + Python packages\n",
        "!apt-get update -y\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "!pip install -q pyspark==3.4.1 pyarrow dask[complete] fastparquet pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff543251",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff543251",
        "outputId": "ef6f8238-a2d2-43f6-e9c5-60e6c2888188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Task1_BigData_PySpark_and_Dask_Notebook.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile Task1_BigData_PySpark_and_Dask_Notebook.py\n",
        "# ===============================\n",
        "# TASK 1 - Big Data Analysis\n",
        "# PySpark + Dask pipelines\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create data directory\n",
        "DATA_DIR = Path('data_big')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# -------- Dataset Acquisition --------\n",
        "import urllib.request\n",
        "\n",
        "sample_csv = DATA_DIR / '311_sample.csv'\n",
        "if not sample_csv.exists():\n",
        "    try:\n",
        "        url = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$limit=50000'\n",
        "        urllib.request.urlretrieve(url, sample_csv)\n",
        "        print('Downloaded sample dataset:', sample_csv)\n",
        "    except Exception as e:\n",
        "        print(\"Download failed, fallback to synthetic data\", e)\n",
        "else:\n",
        "    print(\"Sample already exists:\", sample_csv)\n",
        "\n",
        "# Synthetic dataset\n",
        "def synthesize_dataset(csv_path, n_rows=100000):\n",
        "    import random, csv, datetime\n",
        "    categories = ['A','B','C','D']\n",
        "    locations = ['North','South','East','West']\n",
        "    if csv_path.exists():\n",
        "        return\n",
        "    with open(csv_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['event_id','ts','category','location','value1','value2'])\n",
        "        start = datetime.datetime(2020,1,1)\n",
        "        for i in range(n_rows):\n",
        "            ts = start + datetime.timedelta(seconds=random.randint(0, 60*60*24*365))\n",
        "            writer.writerow([i+1, ts.isoformat(), random.choice(categories),\n",
        "                             random.choice(locations),\n",
        "                             random.random()*1000, random.gauss(50,20)])\n",
        "\n",
        "synthetic_csv = DATA_DIR / 'synthetic_events.csv'\n",
        "synthesize_dataset(synthetic_csv, n_rows=50000)\n",
        "\n",
        "print(\"Data ready at:\", DATA_DIR)\n",
        "\n",
        "# -------- PySpark Pipeline --------\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.functions import col, to_timestamp, count, avg\n",
        "    spark = SparkSession.builder.appName(\"BigDataTask1\").getOrCreate()\n",
        "    file_to_read = str(synthetic_csv if synthetic_csv.exists() else sample_csv)\n",
        "\n",
        "    df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(file_to_read)\n",
        "    df = df.withColumn(\"ts\", to_timestamp(col(\"ts\")))\n",
        "\n",
        "    for c in [\"value1\",\"value2\"]:\n",
        "        if c in df.columns:\n",
        "            df = df.withColumn(c, col(c).cast(\"double\"))\n",
        "\n",
        "    agg = df.groupBy(\"category\",\"location\").agg(\n",
        "        count(\"*\").alias(\"n_events\"),\n",
        "        avg(\"value1\").alias(\"avg_value1\"),\n",
        "        avg(\"value2\").alias(\"avg_value2\")\n",
        "    )\n",
        "    agg.show()\n",
        "    out_dir = DATA_DIR / \"pyspark_output\"\n",
        "    agg.repartition(1).write.mode(\"overwrite\").parquet(str(out_dir))\n",
        "    spark.stop()\n",
        "except Exception as e:\n",
        "    print(\"PySpark section skipped:\", e)\n",
        "\n",
        "# -------- Dask Pipeline --------\n",
        "try:\n",
        "    import dask.dataframe as dd\n",
        "    from dask.distributed import Client, LocalCluster\n",
        "    cluster = LocalCluster(n_workers=2, threads_per_worker=1)\n",
        "    client = Client(cluster)\n",
        "\n",
        "    csv_path = str(synthetic_csv if synthetic_csv.exists() else sample_csv)\n",
        "    ddf = dd.read_csv(csv_path, assume_missing=True)\n",
        "    ddf[\"ts\"] = dd.to_datetime(ddf[\"ts\"], errors=\"coerce\")\n",
        "    for c in [\"value1\",\"value2\"]:\n",
        "        if c in ddf.columns:\n",
        "            ddf[c] = dd.to_numeric(ddf[c], errors=\"coerce\")\n",
        "\n",
        "    agg_ddf = ddf.groupby([\"category\",\"location\"]).agg({\n",
        "        \"event_id\":\"count\",\n",
        "        \"value1\":\"mean\",\n",
        "        \"value2\":\"mean\"\n",
        "    }).rename(columns={\"event_id\":\"n_events\",\"value1\":\"avg_value1\",\"value2\":\"avg_value2\"})\n",
        "\n",
        "    print(agg_ddf.compute().head())\n",
        "    out_parquet = DATA_DIR / \"dask_output\"\n",
        "    agg_ddf.to_parquet(str(out_parquet), engine=\"fastparquet\")\n",
        "    client.close()\n",
        "except Exception as e:\n",
        "    print(\"Dask section skipped:\", e)\n",
        "\n",
        "# -------- Report --------\n",
        "report_path = DATA_DIR / \"summary_report.txt\"\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(\"Big Data Analysis - Task 1\\n\")\n",
        "    f.write(\"Data stored in: \" + str(DATA_DIR) + \"\\n\")\n",
        "    f.write(\"Outputs in pyspark_output/ and dask_output/\\n\")\n",
        "print(\"Report written:\", report_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d2c3fe07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2c3fe07",
        "outputId": "07ce51e1-2a1b-4074-cb11-17d2bcb340cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded sample dataset: data_big/311_sample.csv\n",
            "Data ready at: data_big\n",
            "+--------+--------+--------+------------------+------------------+\n",
            "|category|location|n_events|        avg_value1|        avg_value2|\n",
            "+--------+--------+--------+------------------+------------------+\n",
            "|       B|   North|    3203|504.91970344847596| 49.94472190349554|\n",
            "|       D|   North|    3072| 499.0922068270466|49.998765099796096|\n",
            "|       D|    West|    3127|496.39151355872656| 49.75940751819461|\n",
            "|       D|    East|    3162| 497.9248178962062|50.001359167035886|\n",
            "|       B|   South|    3138|502.51777789874376| 50.21172531422503|\n",
            "|       C|    West|    2954| 505.6136827503561| 50.71398613751749|\n",
            "|       B|    East|    3113| 503.7441174029004| 50.40117699733774|\n",
            "|       C|    East|    3246|  507.290271622896|50.124099977081926|\n",
            "|       B|    West|    3193|  498.666425248334|50.272355301101236|\n",
            "|       A|   South|    3152|497.75147478181674|49.940089394260625|\n",
            "|       A|    West|    3149|495.14304969039716| 49.56886512491903|\n",
            "|       A|    East|    3113| 499.4377100016511| 49.79324344467757|\n",
            "|       A|   North|    3183| 491.9915453051955| 49.64325796167831|\n",
            "|       C|   South|    3060|493.29133730248657|49.915417583141036|\n",
            "|       D|   South|    3022|  504.793553171896| 50.20430937734412|\n",
            "|       C|   North|    3113| 502.5644362453508|50.219850762332115|\n",
            "+--------+--------+--------+------------------+------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:33551\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:37389'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:33557'\n",
            "INFO:distributed.nanny:Worker process 2725 was killed by signal 2\n",
            "WARNING:distributed.nanny:Restarting worker\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask section skipped: 'NoneType' object has no attribute 'sc'\n",
            "Report written: data_big/summary_report.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%run Task1_BigData_PySpark_and_Dask_Notebook.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fe8f291a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe8f291a",
        "outputId": "7891e5e6-983d-46ad-be82-65538e270842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 34532\n",
            "-rw-r--r-- 1 root root 31816165 Oct  4 10:17 311_sample.csv\n",
            "drwxr-xr-x 2 root root     4096 Oct  4 10:18 pyspark_output\n",
            "-rw-r--r-- 1 root root       96 Oct  4 10:20 summary_report.txt\n",
            "-rw-r--r-- 1 root root  3532012 Oct  4 10:17 synthetic_events.csv\n",
            "Big Data Analysis - Task 1\n",
            "Data stored in: data_big\n",
            "Outputs in pyspark_output/ and dask_output/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls -l data_big\n",
        "!head -n 20 data_big/summary_report.txt\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}